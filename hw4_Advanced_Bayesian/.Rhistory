#Let’s make a data frame showing the page numbers of ten articles in a festschrift.
#First we will create three vectors. The first one has the authors' names:
authors <- c("Adams", "Baker", "Collins", "Davidson", "Evans", "Freeman", "Godfrey", "Halvorsen", "Indigo", "Jespersen")
#The second vector contains the first page of the article by each author:
firstpage <- c(1, 12, 54, 63, 66, 90, 101, 126, 201, 212)
#The third vector contains the last page of the article by each author:
lastpage <- c(11, 53, 62, 65, 89, 100, 125, 200, 211, 250)
#Now we combine the three vectors we have created into a data frame:
festschrift <- data.frame(authors, firstpage, lastpage)
# [1]   1  12  54  63  66  90 101 126 201 212
#Let’s say that we want to know the length of each article:
festschrift$length=(festschrift$lastpage-festschrift$firstpage)+1
#Note that our data frame now has a new column.
festschrift
#2      Baker        12       53     42
#3    Collins        54       62      9
#4   Davidson        63       65      3
#5      Evans        66       89     24
#6    Freeman        90      100     11
#7    Godfrey       101      125     25
#8  Halvorsen       126      200     75
#9     Indigo       201      211     11
#10 Jespersen       212      250     39
#We can now apply various functions to the length column
summary(festschrift$length)
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
#    3.0    11.0    17.5    25.0    35.5    75.0
sd(festschrift$length)
#[1] 21.89368
sort(festschrift$length)
# [1]  3  9 11 11 11 24 25 39 42 75
plot(festschrift$length)
hist(festschrift$length)
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read.csv(file.choose(), sep=";")
hist(festschrift$length, bins = nclass.Sturges(festschrift$length))
festschrift %>%
ggplot(aes(length))+
geom_histogram(bins = nclass.Sturges(festschrift$length))
library("tidyverse", lib.loc="~/R/win-library/3.5")
festschrift %>%
ggplot(aes(length))+
geom_histogram(bins = nclass.Sturges(festschrift$length))
hist(festschrift$length)
hist(festschrift$length, breaks = "Sturges")
festschrift %>%
ggplot(aes(length))+
geom_histogram(bins = nclass.Sturges(festschrift$length))
hist(festschrift$length, breaks = "Sturges")
festschrift %>%
ggplot(aes(length))+
geom_histogram(bins = nclass.Sturges(festschrift$length))
hist(festschrift$length, breaks = "Sturges")
hist(festschrift$length, breaks = "Sturges", main = paste("Histogram of length of articles in festschrift"))
hist(festschrift$length, breaks = "Sturges", xname = "length of articles in festschrift")
hist(festschrift$length, breaks = "Sturges", main = paste("Histogram of length of articles in festschrift"), xlab = "length of articles in festschrift")
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv('HandbookCogLx', sep=";")
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv('HandbookCogLx')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv('HandbookCogLx.csv', sep=";")
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv('HandbookCogLx.csv')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv('C:\My\studies\UIT\Qunatitative Linguistics\CW\02\HandbookCogLx.csv')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv(r'C:\My\studies\UIT\Qunatitative Linguistics\CW\02\HandbookCogLx.csv')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv('HandbookCogLx.csv')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv('C:/My/studies/UIT/Qunatitative Linguistics/CW/02/HandbookCogLx.csv')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv('C:/My/studies/UIT/Qunatitative Linguistics/CW/02/HandbookCogLx.csv')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read_csv(choose.file(), sep=';')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read.csv(choose.file(), sep=';')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read.csv(choose_file(), sep=';')
#Now try your hand at a real dataset. The page numbers for articles in the Handbook of Cognitive Linguistics published in 2015 are in the file called HandbookCogLx.csv.
#First save that file somewhere where you can find it.
#Let’s call the table with that information coglx and import the data this way:
coglx <- read.csv(file.choose(), sep=';')
View(coglx)
#This command will open a browser window on your computer and you should find the file HandbookCogLx.csv and click on it.
#Now see if you can create a new column showing the length of each article and plot the histogram, and find the mean and standard deviation, etc.
coglx %>%
mutate(length = lastpage - firstpage + 1)
View(coglx)
#This command will open a browser window on your computer and you should find the file HandbookCogLx.csv and click on it.
#Now see if you can create a new column showing the length of each article and plot the histogram, and find the mean and standard deviation, etc.
coglx %>%
mutate(Length = LastPage - FirstPage + 1)
View(coglx)
#This command will open a browser window on your computer and you should find the file HandbookCogLx.csv and click on it.
#Now see if you can create a new column showing the length of each article and plot the histogram, and find the mean and standard deviation, etc.
coglx %>%
mutate(Length = LastPage - FirstPage + 1) -> coglx
summarise(coglx$length)
summarise(coglx$Length)
summary(coglx$Length)
sd(coglx$Length)
library('Rling')
install.packages('Rling')
library(Rling)
install.packages('rling')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
p = read_csv('3_2.csv')
setwd("C:/My/studies/HSE/R_Data_analysis/hw3_bayesian_inference_again")
### 3.2
```{r pressure, echo=FALSE}
p = read_csv('3_2.csv')
setwd("C:/My/studies/HSE/R_Data_analysis/hw3_bayesian_inference_again")
p = read_csv('3_2.csv')
library(tidyverse)
read_csv('cw4task1.csv')
read_csv('cw4task1.csv')
setwd("C:/My/studies/HSE/R_Data_analysis/hw4_Advanced_Bayesian")
read_csv('cw4task1.csv')
cities <- read_csv('cw4task1.csv')
View(cities)
cities %>%
select(city == 'Auckland')
cities %>%
select(cities, city == 'Auckland')
cities %>%
select(city == 'Auckland')
cities %>%
select(city)
cities %>%
fulter(city == 'Auckland')
cities %>%
filter(city == 'Auckland')
cities %>%
filter(city == 'Auckland') %>%
count(events == 'Rain')
cities %>%
filter(city == 'Auckland') %>%
count(events == 'Rain')[TRUE]
cities %>%
filter(city == 'Auckland') %>%
count(events == 'Rain')[1]
cities %>%
filter(city == 'Auckland') %>%
count(events == 'Rain')
cities %>%
filter(city == 'Auckland') %>%
count(events == 'Rain') -> rain_Auckland
View(rain_Auckland)
cities %>%
filter(city == 'Auckland') %>%
count(events == 'Rain Thunderstorm') -> rain_thunderstorm_Auckland
View(rain_thunderstorm_Auckland)
cities %>%
filter(city == 'San_Diego') %>%
count(events == 'Rain') -> rain_San_Diego
View(rain_San_Diego)
cities %>%
filter(city == 'San Diego') %>%
count(events == 'Rain') -> rain_San_Diego
View(rain_San_Diego)
cities %>%
filter(city == 'San Diego') %>%
count(events == 'Rain Thunderstorm') -> rain_thunderstorm_San_Diego
View(rain_thunderstorm_San_Diego)
cities %>%
filter(city == 'Auckland') %>%
count(events == 'Rain , Thunderstorm') -> rain_thunderstorm_Auckland
cities %>%
filter(city == 'San Diego') %>%
count(events == 'Rain , Thunderstorm') -> rain_thunderstorm_San_Diego
View(rain_Auckland)
rain_Auckland %>%
sum(n)
rain_Auckland[2]
sum(rain_Auckland[2])
View(rain_Auckland)
rain_Auckland[2,2]
rain_Auckland[2,2]/sum(rain_Auckland[2])
p_r_A <- rain_Auckland[2,2]/sum(rain_Auckland[2])
p_r_S <- rain_San_Diego[2,2]/sum(rain_San_Diego[2])
p_t_S <- rain_thunderstorm_San_Diego[2,2]/sum(rain_thunderstorm_San_Diego[2])
p_t_A <- rain_thunderstorm_Auckland[2,2]/sum(rain_thunderstorm_Auckland[2])
L_S <- p_t_S * p_r_S
View(L_S)
L_A <- p_t_A * p_r_A
P <- L_S/L_A
View(P)
data <- read_csv('hw4_caretta.csv')
View(data)
mean(data$vowel.dur, trim =0.1)
sd(data$vowel.dur)
rnorm(mean = mean(data$vowel.dur, trim =0.1), sd = sd(data$vowel.dur))
rnorm(n =100, mean = mean(data$vowel.dur, trim =0.1), sd = sd(data$vowel.dur))
pnorm(n = 100, mean = mean(data$vowel.dur, trim =0.1), sd = sd(data$vowel.dur))
pnorm(mean = mean(data$vowel.dur, trim =0.1), sd = sd(data$vowel.dur))
data %>%
filter(speaker == 'brs02')
data %>%
filter(speaker == 'brs02') %>%
density()
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
density()
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur)
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
as.numeric()
data %>%
filter(speaker == 'brs02') %>%
mutate(dur = as.numeric(vowel.dur))
data %>%
filter(speaker == 'brs02') %>%
hist()
data %>%
filter(speaker == 'brs02') %>%
round()
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
round()
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
round() %>%
hist()
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
round()
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
round() %>%
as.numeric()
plot(dnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur)))
ggplot2(dnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur)))
ggplot(aes(dnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur))))+geom_density()
data %>%
ggplot(aes(vowel.dur))+
geom_density(alpha = 0.5)
dnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur)))
dnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur))
dnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur))
rnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur))
ggplot(aes(rnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur)))+geom_density()
ggplot(aes(rnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur)))+geom_density())
ggplot(aes(rnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur))))+geom_density()
rnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur)) %>%
ggplot(aes())+
geom_density(alpha = 0.5)
df(rnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur))) %>%
ggplot(aes())+
geom_density(alpha = 0.5)
df(n = rnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur))) %>%
ggplot(aes(n))+
geom_density(alpha = 0.5)
df(x = rnorm(100, mean(data$vowel.dur, trim =0.1), sd(data$vowel.dur))) %>%
ggplot(aes(x))+
geom_density(alpha = 0.5)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylab("") +
scale_y_continuous(breaks = NULL)
sd <- sd(data$vowel.dur)
m <- mean(data$vowel.dur, trim =0.1)
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
stat_function(fun = dnorm, n = 101, args = list(mean = m, sd = sd)) + ylab("") +
scale_y_continuous(breaks = NULL)
ggplot(data = data.frame(x = c(-1000, 1000)), aes(x)) +
stat_function(fun = dnorm, n = 101, args = list(mean = m, sd = sd)) + ylab("") +
scale_y_continuous(breaks = NULL)
ggplot(data = data.frame(x = c(-100, 250)), aes(x)) +
stat_function(fun = dnorm, n = 101, args = list(mean = m, sd = sd)) + ylab("") +
scale_y_continuous(breaks = NULL)
ggplot(data = data.frame(x = c(-100, 250)), aes(x)) +
stat_function(fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab("") +
scale_y_continuous(breaks = NULL)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab("") +
scale_y_continuous(breaks = NULL)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur, color='red'), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x), color='red') +
stat_function(fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x), color='red') +
stat_function(fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)+
geom_label(aes('boo'))
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)+
geom_label(aes(label = 'boo'))
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) + ylab('density') +
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)+
labs(x = 'vowel duration', y = 'density',
caption = 'boo')
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) +
ylab('density') +
xlab('vowel duration')
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) +
ylab('density') +
xlab('vowel duration')+
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red', label = 'prior'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) +
ylab('density') +
xlab('vowel duration')+
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x, label = 'boo')) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) +
ylab('density') +
xlab('vowel duration')+
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x, lable = 'boo')) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) +
ylab('density') +
xlab('vowel duration')+
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data.frame(x = c(-100, 250)) %>%
ggplot(aes(x)) +
stat_function(aes(color='red'), fun = dnorm, n = 100, args = list(mean = m, sd = sd)) +
ylab('density') +
xlab('vowel duration')+
scale_y_continuous(breaks = NULL)+
geom_density(data = data, aes(vowel.dur), alpha = 0.5)
data %>%
filter(speaker = 'brs02')
data %>%
filter(speaker == 'brs02')
data %>%
filter(speaker == 'brs02') %>%
summarise()
data %>%
filter(speaker == 'brs02') %>%
summary()
data %>%
filter(speaker == 'brs02') %>%
mean() -> m_brs02
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
mean() -> m_brs02
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur)
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
mean()
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
as.single()
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur)[1]
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur)
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) -> brs02
brs02[1]
brs02[1,]
as.numeric(brs02[1,])
data %>%
filter(speaker == 'tt01') %>%
select(vowel.dur) -> tt01
rm(m_brs02)
m_prior <- mean(brs02, trim =0.1)
m_prior <- mean(brs02, trim = 0.1)
sd_prior <- sd(brs02)
m <- mean(data$vowel.dur, trim =0.1)
data %>%
filter(speaker == 'brs02') %>%
select(vowel.dur) %>%
mean(, trim = 0.1)
filter(data, speaker == 'brs02')
filter(data, speaker == 'brs02')$vowel.dur
m_prior <- mean(filter(data, speaker == 'brs02')$vowel.dur, trim = 0.1)
sd_prior <- sd(filter(data, speaker == 'brs02')
sd_prior <- sd(filter(data, speaker == 'brs02'))
sd_prior <- sd(filter(data, speaker == 'brs02')$vowel.dur)
m_post = weighted.mean(c(m_prior, m_speaker), c(1/sd_prior, 1/sd_speaker))
m_brs02 <- mean(filter(data, speaker == 'brs02')$vowel.dur, trim = 0.1)
sd_brs02 <- sd(filter(data, speaker == 'brs02')$vowel.dur)
m_post = weighted.mean(c(m, m_brs02), c(1/sd, 1/sd_brs02))
data <- read_csv('hw4_caretta.csv')
m <- mean(data$vowel.dur, trim =0.1)
sd <- sd(data$vowel.dur)
m_brs02 <- mean(filter(data, speaker == 'brs02')$vowel.dur, trim = 0.1)
sd_brs02 <- sd(filter(data, speaker == 'brs02')$vowel.dur)
m_post = weighted.mean(c(m, m_brs02), c(1/sd, 1/sd_brs02))
sd_post = (1/sd+1/sd_brs02)^(-1)
data.frame(sd = sd_post)
data.frame(mean = m_post, sd = sd_post)
m_tt01 <- mean(filter(data, speaker == 'tt01')$vowel.dur, trim = 0.1)
sd_tt01 <- sd(filter(data, speaker == 'tt01')$vowel.dur)
m_post <- weighted.mean(c(m_prior, m_tt01), c(1/sd_prior, 1/sd_tt01))
m_post <- weighted.mean(c(m, m_tt01), c(1/sd, 1/sd_tt01))
sd_post <- (1/sd+1/sd_tt01)^(-1)
cred_int_l <- qnorm(.025, m_post, sd_post)
cred_int_h <- qnorm(.975, m_post, sd_post)
data.frame(lower = cred_int_l, higher = cred_int_h)
library(tidyverse)
eng <- read_csv('C:/My/studies/UIT/FLA/code/FLA/result_english_personal.csv')
tibble(eng)
View(eng)
